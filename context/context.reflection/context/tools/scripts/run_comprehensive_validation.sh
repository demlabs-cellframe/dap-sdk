#!/bin/bash

# üß™ Comprehensive Template Validation Suite
# Master script –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö validation tests

set -e  # Exit on any error

echo "üöÄ –ó–∞–ø—É—Å–∫ Comprehensive Template Validation Suite"
echo "=================================================="

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
CONTEXT_ROOT="${1:-context/context.reflection/context}"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
LOG_DIR="${SCRIPT_DIR}/../logs"
TIMESTAMP=$(date '+%Y%m%d_%H%M%S')

# Create logs directory
mkdir -p "$LOG_DIR"

echo -e "${BLUE}üìÅ Context Root: $CONTEXT_ROOT${NC}"
echo -e "${BLUE}üìù Logs Directory: $LOG_DIR${NC}"
echo ""

# Check if context root exists
if [ ! -d "$CONTEXT_ROOT" ]; then
    echo -e "${RED}‚ùå Error: Context directory '$CONTEXT_ROOT' not found${NC}"
    exit 1
fi

# Function to run test and capture results
run_test() {
    local test_name="$1"
    local test_command="$2"
    local log_file="$LOG_DIR/${test_name}_${TIMESTAMP}.log"
    
    echo -e "${BLUE}üîç Running: $test_name${NC}"
    
    if eval "$test_command" > "$log_file" 2>&1; then
        echo -e "${GREEN}‚úÖ $test_name: PASSED${NC}"
        return 0
    else
        echo -e "${RED}‚ùå $test_name: FAILED${NC}"
        echo -e "${YELLOW}   Log: $log_file${NC}"
        return 1
    fi
}

# Function to check Python availability
check_python() {
    if command -v python3 &> /dev/null; then
        echo "python3"
    elif command -v python &> /dev/null; then
        echo "python"
    else
        echo ""
    fi
}

# Check dependencies
echo -e "${BLUE}üîß Checking dependencies...${NC}"

PYTHON_CMD=$(check_python)
if [ -z "$PYTHON_CMD" ]; then
    echo -e "${RED}‚ùå Python not found. Please install Python 3.x${NC}"
    exit 1
fi

echo -e "${GREEN}‚úÖ Python found: $PYTHON_CMD${NC}"

# Check if jq is available for JSON validation
if command -v jq &> /dev/null; then
    JQ_AVAILABLE=true
    echo -e "${GREEN}‚úÖ jq found for JSON validation${NC}"
else
    JQ_AVAILABLE=false
    echo -e "${YELLOW}‚ö†Ô∏è  jq not found - JSON validation will be limited${NC}"
fi

echo ""

# Initialize counters
TOTAL_TESTS=0
PASSED_TESTS=0
FAILED_TESTS=0

# Test 1: JSON Structure Validation
echo -e "${BLUE}üìã Phase 1: JSON Structure Validation${NC}"
if [ "$JQ_AVAILABLE" = true ]; then
    TOTAL_TESTS=$((TOTAL_TESTS + 1))
    if run_test "json_structure_validation" "find '$CONTEXT_ROOT' -name '*.json' -exec jq . {} \; > /dev/null"; then
        PASSED_TESTS=$((PASSED_TESTS + 1))
    else
        FAILED_TESTS=$((FAILED_TESTS + 1))
    fi
else
    echo -e "${YELLOW}‚ö†Ô∏è  Skipping JSON validation (jq not available)${NC}"
fi

# Test 2: Python Template Validation
echo -e "${BLUE}üìã Phase 2: Template Validation${NC}"
TOTAL_TESTS=$((TOTAL_TESTS + 1))
if run_test "template_validation" "$PYTHON_CMD '$SCRIPT_DIR/validate_all_templates.py' '$CONTEXT_ROOT'"; then
    PASSED_TESTS=$((PASSED_TESTS + 1))
else
    FAILED_TESTS=$((FAILED_TESTS + 1))
fi

# Test 3: Edge Case Testing
echo -e "${BLUE}üß™ Phase 3: Edge Case Testing${NC}"
TOTAL_TESTS=$((TOTAL_TESTS + 1))
if run_test "edge_case_testing" "$PYTHON_CMD '$SCRIPT_DIR/run_edge_case_tests.py' '$CONTEXT_ROOT'"; then
    PASSED_TESTS=$((PASSED_TESTS + 1))
else
    FAILED_TESTS=$((FAILED_TESTS + 1))
fi

# Test 4: File Structure Validation
echo -e "${BLUE}üìÅ Phase 4: File Structure Validation${NC}"
TOTAL_TESTS=$((TOTAL_TESTS + 1))

# Check for required directories and files
REQUIRED_DIRS=(
    "modules/methodologies"
    "tools"
    "docs"
)

REQUIRED_FILES=(
    "modules/methodologies/universal_optimization.json"
    "tools/workflow_recommendation_engine.json"
    "docs/–º–∞—Ç—Ä–∏—Ü–∞_–≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏_—à–∞–±–ª–æ–Ω–æ–≤.md"
)

STRUCTURE_OK=true

for dir in "${REQUIRED_DIRS[@]}"; do
    if [ ! -d "$CONTEXT_ROOT/$dir" ]; then
        echo -e "${RED}‚ùå Missing required directory: $dir${NC}"
        STRUCTURE_OK=false
    fi
done

for file in "${REQUIRED_FILES[@]}"; do
    if [ ! -f "$CONTEXT_ROOT/$file" ]; then
        echo -e "${RED}‚ùå Missing required file: $file${NC}"
        STRUCTURE_OK=false
    fi
done

if [ "$STRUCTURE_OK" = true ]; then
    echo -e "${GREEN}‚úÖ file_structure_validation: PASSED${NC}"
    PASSED_TESTS=$((PASSED_TESTS + 1))
else
    echo -e "${RED}‚ùå file_structure_validation: FAILED${NC}"
    FAILED_TESTS=$((FAILED_TESTS + 1))
fi

# Test 5: Markdown Link Validation
echo -e "${BLUE}üìù Phase 5: Markdown Link Validation${NC}"
TOTAL_TESTS=$((TOTAL_TESTS + 1))

# Simple markdown link check
MARKDOWN_OK=true
while IFS= read -r -d '' file; do
    # Extract markdown links [text](url)
    if grep -P '\[([^\]]+)\]\(([^)]+)\)' "$file" > /dev/null; then
        # Check for obviously broken links (very basic check)
        if grep -P '\[([^\]]+)\]\(\s*\)' "$file" > /dev/null; then
            echo -e "${RED}‚ùå Empty link found in: $file${NC}"
            MARKDOWN_OK=false
        fi
    fi
done < <(find "$CONTEXT_ROOT" -name "*.md" -print0)

if [ "$MARKDOWN_OK" = true ]; then
    echo -e "${GREEN}‚úÖ markdown_link_validation: PASSED${NC}"
    PASSED_TESTS=$((PASSED_TESTS + 1))
else
    echo -e "${RED}‚ùå markdown_link_validation: FAILED${NC}"
    FAILED_TESTS=$((FAILED_TESTS + 1))
fi

# Test 6: Performance Benchmark
echo -e "${BLUE}‚ö° Phase 6: Performance Benchmark${NC}"
TOTAL_TESTS=$((TOTAL_TESTS + 1))

START_TIME=$(date +%s.%N)

# Simulate template operations
FILE_COUNT=$(find "$CONTEXT_ROOT" -type f \( -name "*.json" -o -name "*.md" \) | wc -l)
JSON_COUNT=$(find "$CONTEXT_ROOT" -name "*.json" | wc -l)
MD_COUNT=$(find "$CONTEXT_ROOT" -name "*.md" | wc -l)

END_TIME=$(date +%s.%N)
EXECUTION_TIME=$(echo "$END_TIME - $START_TIME" | bc)

echo -e "${BLUE}üìä Performance Results:${NC}"
echo -e "   Files processed: $FILE_COUNT (JSON: $JSON_COUNT, MD: $MD_COUNT)"
echo -e "   Execution time: ${EXECUTION_TIME}s"

# Check if performance is acceptable (< 5 seconds for basic operations)
if (( $(echo "$EXECUTION_TIME < 5.0" | bc -l) )); then
    echo -e "${GREEN}‚úÖ performance_benchmark: PASSED${NC}"
    PASSED_TESTS=$((PASSED_TESTS + 1))
else
    echo -e "${RED}‚ùå performance_benchmark: FAILED (too slow)${NC}"
    FAILED_TESTS=$((FAILED_TESTS + 1))
fi

# Generate comprehensive report
echo ""
echo "================================================================"
echo -e "${BLUE}üìä COMPREHENSIVE VALIDATION REPORT${NC}"
echo "================================================================"

echo -e "–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:"
echo -e "  –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: $TOTAL_TESTS"
echo -e "  ${GREEN}‚úÖ –ü—Ä–æ—à–ª–∏: $PASSED_TESTS${NC}"
echo -e "  ${RED}‚ùå –ü—Ä–æ–≤–∞–ª–µ–Ω—ã: $FAILED_TESTS${NC}"

if [ $TOTAL_TESTS -gt 0 ]; then
    SUCCESS_RATE=$(echo "scale=1; $PASSED_TESTS * 100 / $TOTAL_TESTS" | bc)
    echo -e "  üìä –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: ${SUCCESS_RATE}%"
fi

echo ""
echo -e "${BLUE}üìÅ Logs —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: $LOG_DIR${NC}"

# List log files
if [ -d "$LOG_DIR" ]; then
    echo -e "${BLUE}üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –ª–æ–≥–∏:${NC}"
    for log_file in "$LOG_DIR"/*_${TIMESTAMP}.log; do
        if [ -f "$log_file" ]; then
            echo -e "   $(basename "$log_file")"
        fi
    done
fi

echo ""

# Final recommendations
if [ $FAILED_TESTS -eq 0 ]; then
    echo -e "${GREEN}üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏ —É—Å–ø–µ—à–Ω–æ! –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.${NC}"
    exit 0
else
    echo -e "${RED}‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:${NC}"
    echo -e "   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
    echo -e "   2. –ò—Å–ø—Ä–∞–≤—å—Ç–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –ø–µ—Ä–µ–¥ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–æ–º"
    echo -e "   3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç—ã –ø–æ–≤—Ç–æ—Ä–Ω–æ –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π"
    exit 1
fi 